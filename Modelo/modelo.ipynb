{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>balance</th>\n",
       "      <th>day</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>job_blue-collar</th>\n",
       "      <th>job_entrepreneur</th>\n",
       "      <th>job_housemaid</th>\n",
       "      <th>...</th>\n",
       "      <th>month_jun</th>\n",
       "      <th>month_mar</th>\n",
       "      <th>month_may</th>\n",
       "      <th>month_nov</th>\n",
       "      <th>month_oct</th>\n",
       "      <th>month_sep</th>\n",
       "      <th>poutcome_other</th>\n",
       "      <th>poutcome_success</th>\n",
       "      <th>poutcome_unknown</th>\n",
       "      <th>y_yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>2143</td>\n",
       "      <td>5</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>1506</td>\n",
       "      <td>5</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>198</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  balance  day  duration  campaign  pdays  previous  job_blue-collar  \\\n",
       "0   58     2143    5       261         1     -1         0                0   \n",
       "1   44       29    5       151         1     -1         0                0   \n",
       "2   33        2    5        76         1     -1         0                0   \n",
       "3   47     1506    5        92         1     -1         0                1   \n",
       "4   33        1    5       198         1     -1         0                0   \n",
       "\n",
       "   job_entrepreneur  job_housemaid  ...  month_jun  month_mar  month_may  \\\n",
       "0                 0              0  ...          0          0          1   \n",
       "1                 0              0  ...          0          0          1   \n",
       "2                 1              0  ...          0          0          1   \n",
       "3                 0              0  ...          0          0          1   \n",
       "4                 0              0  ...          0          0          1   \n",
       "\n",
       "   month_nov  month_oct  month_sep  poutcome_other  poutcome_success  \\\n",
       "0          0          0          0               0                 0   \n",
       "1          0          0          0               0                 0   \n",
       "2          0          0          0               0                 0   \n",
       "3          0          0          0               0                 0   \n",
       "4          0          0          0               0                 0   \n",
       "\n",
       "   poutcome_unknown  y_yes  \n",
       "0                 1      0  \n",
       "1                 1      0  \n",
       "2                 1      0  \n",
       "3                 1      0  \n",
       "4                 1      0  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Verificar si el archivo existe\n",
    "file_path = '../Limpieza/data_dummies.csv'\n",
    "\n",
    "data = pd.read_csv(file_path, sep=';')\n",
    " \n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45211, 43)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Características seleccionadas: ['duration']\n",
      "Mean Squared Error: 0.09996054524618525\n",
      "Coeficiente de determinación R^2: 0.05777948465198668\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Separar características (X) y variable objetivo (y)\n",
    "X = data.drop(columns=['y_yes'])\n",
    "y = data['y_yes']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Escalar las características (Lasso es sensible a la escala de las variables)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Crear y ajustar el modelo Lasso\n",
    "lasso = Lasso(alpha=0.1)  # Ajusta alpha para controlar la regularización\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Imprimir las características seleccionadas (con coeficiente distinto de cero)\n",
    "selected_features = [feature for feature, coef in zip(X.columns, lasso.coef_) if coef != 0]\n",
    "print(\"Características seleccionadas:\", selected_features)\n",
    "\n",
    "# Evaluar el modelo Lasso\n",
    "y_pred = lasso.predict(X_test_scaled)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Coeficiente de determinación R^2:\", lasso.score(X_test_scaled, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation R^2 scores: [0.05988667 0.05702819 0.05630836 0.05735894 0.05613545 0.05672125\n",
      " 0.05744418 0.05903862 0.06295517 0.06212107]\n",
      "Mean Cross-validation R^2 score: 0.05849978976333855\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Separar características (X) y variable objetivo (y)\n",
    "X = data.drop(columns=['y_yes'])  # Características\n",
    "y = data['y_yes']  # Variable objetivo\n",
    "\n",
    "# Crear un pipeline que incluya el escalado y el modelo Lasso\n",
    "pipeline = make_pipeline(StandardScaler(), Lasso(alpha=0.1))\n",
    "\n",
    "# Definir el objeto KFold para 5 particiones (por ejemplo)\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Realizar la validación cruzada\n",
    "scores = cross_val_score(pipeline, X, y, cv=kf, scoring='r2')\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(\"Cross-validation R^2 scores:\", scores)\n",
    "print(\"Mean Cross-validation R^2 score:\", scores.mean())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation accuracy scores: [0.90468819 0.90776377 0.9048883  0.87856669 0.88608715 0.88365406\n",
      " 0.85733245 0.87303694 0.88564477 0.85202389]\n",
      "Mean cross-validation accuracy: 0.8833686200355881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# Crear el modelo de regresión logística\n",
    "logistic_regression = LogisticRegression()\n",
    "\n",
    "# Realizar la validación cruzada con la métrica de precisión (accuracy)\n",
    "scores = cross_val_score(logistic_regression, X, y, cv=10, scoring='accuracy')\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(\"Cross-validation accuracy scores:\", scores)\n",
    "print(\"Mean cross-validation accuracy:\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable: age, Coeficiente: -0.01703285836807898\n",
      "Variable: balance, Coeficiente: 1.7631351319968e-05\n",
      "Variable: day, Coeficiente: -0.010300898319114715\n",
      "Variable: duration, Coeficiente: 0.0038234833888144575\n",
      "Variable: campaign, Coeficiente: -0.3844342717945095\n",
      "Variable: pdays, Coeficiente: 0.0023113371798994154\n",
      "Variable: previous, Coeficiente: -0.14579819447206896\n",
      "Variable: job_blue-collar, Coeficiente: -0.2884440152805934\n",
      "Variable: job_entrepreneur, Coeficiente: -0.027931949303613538\n",
      "Variable: job_housemaid, Coeficiente: -0.00605384785894319\n",
      "Variable: job_management, Coeficiente: 0.007488035283181514\n",
      "Variable: job_retired, Coeficiente: 0.182535413423583\n",
      "Variable: job_self-employed, Coeficiente: -0.014380571015067544\n",
      "Variable: job_services, Coeficiente: -0.09613022347680289\n",
      "Variable: job_student, Coeficiente: 0.028846413772819376\n",
      "Variable: job_technician, Coeficiente: -0.08324617623427633\n",
      "Variable: job_unemployed, Coeficiente: 0.005016086284404601\n",
      "Variable: job_unknown, Coeficiente: 0.002231427355482808\n",
      "Variable: marital_married, Coeficiente: -0.19417097513274412\n",
      "Variable: marital_single, Coeficiente: -0.15076311887632524\n",
      "Variable: education_secondary, Coeficiente: -0.30265537667519005\n",
      "Variable: education_tertiary, Coeficiente: 0.05329738091402511\n",
      "Variable: education_unknown, Coeficiente: 0.008323877336254129\n",
      "Variable: default_yes, Coeficiente: -0.022429578789657248\n",
      "Variable: housing_yes, Coeficiente: -0.7576636940526585\n",
      "Variable: loan_yes, Coeficiente: -0.2123435658759606\n",
      "Variable: contact_telephone, Coeficiente: 0.06090219978918603\n",
      "Variable: contact_unknown, Coeficiente: -0.49012678518208513\n",
      "Variable: month_aug, Coeficiente: 0.03466106840004107\n",
      "Variable: month_dec, Coeficiente: 0.03371547488448022\n",
      "Variable: month_feb, Coeficiente: -0.025894379889518333\n",
      "Variable: month_jan, Coeficiente: -0.01638298199234478\n",
      "Variable: month_jul, Coeficiente: -0.08716651295763576\n",
      "Variable: month_jun, Coeficiente: -0.0676225064335464\n",
      "Variable: month_mar, Coeficiente: 0.09543247883174585\n",
      "Variable: month_may, Coeficiente: -0.47870063392122236\n",
      "Variable: month_nov, Coeficiente: -0.06385432607100734\n",
      "Variable: month_oct, Coeficiente: 0.1124415622055625\n",
      "Variable: month_sep, Coeficiente: 0.08589776938550607\n",
      "Variable: poutcome_other, Coeficiente: -0.031737658120258805\n",
      "Variable: poutcome_success, Coeficiente: 0.34874327696348256\n",
      "Variable: poutcome_unknown, Coeficiente: -0.4864270705755285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Asegurarse de que el modelo esté ajustado antes de acceder a coef_\n",
    "logistic_regression.fit(X_train, y_train)\n",
    "\n",
    "# Obtener el coeficiente para cada característica\n",
    "coef = logistic_regression.coef_[0]  # .coef_ es una matriz de 2D; [0] selecciona la primera fila\n",
    "variables = X.columns\n",
    "\n",
    "# Imprimir cada variable con su coeficiente\n",
    "for var, c in zip(variables, coef):\n",
    "    print(f\"Variable: {var}, Coeficiente: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial accuracy on test set: 0.8987061815769103\n",
      "\n",
      "Final model summary:\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                  y_yes   No. Observations:                36168\n",
      "Model:                          Logit   Df Residuals:                    36138\n",
      "Method:                           MLE   Df Model:                           29\n",
      "Date:                Fri, 01 Nov 2024   Pseudo R-squ.:                  0.3374\n",
      "Time:                        17:35:53   Log-Likelihood:                -8603.3\n",
      "converged:                       True   LL-Null:                       -12985.\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "=======================================================================================\n",
      "                          coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------\n",
      "const                  -2.4412      0.111    -22.092      0.000      -2.658      -2.225\n",
      "day                     0.0101      0.003      3.590      0.000       0.005       0.016\n",
      "duration                0.0042   7.26e-05     57.990      0.000       0.004       0.004\n",
      "campaign               -0.0911      0.011     -8.059      0.000      -0.113      -0.069\n",
      "job_blue-collar        -0.3229      0.068     -4.754      0.000      -0.456      -0.190\n",
      "job_entrepreneur       -0.3077      0.132     -2.327      0.020      -0.567      -0.049\n",
      "job_housemaid          -0.4842      0.141     -3.436      0.001      -0.760      -0.208\n",
      "job_management         -0.1284      0.066     -1.948      0.051      -0.258       0.001\n",
      "job_services           -0.1885      0.084     -2.251      0.024      -0.353      -0.024\n",
      "job_student             0.3676      0.110      3.336      0.001       0.152       0.584\n",
      "job_technician         -0.1832      0.064     -2.866      0.004      -0.309      -0.058\n",
      "marital_married        -0.2309      0.042     -5.466      0.000      -0.314      -0.148\n",
      "education_secondary     0.1362      0.061      2.229      0.026       0.016       0.256\n",
      "education_tertiary      0.2788      0.072      3.861      0.000       0.137       0.420\n",
      "housing_yes            -0.6895      0.048    -14.350      0.000      -0.784      -0.595\n",
      "loan_yes               -0.4115      0.066     -6.214      0.000      -0.541      -0.282\n",
      "contact_unknown        -1.6821      0.081    -20.741      0.000      -1.841      -1.523\n",
      "month_aug              -0.7015      0.087     -8.097      0.000      -0.871      -0.532\n",
      "month_dec               0.6533      0.191      3.420      0.001       0.279       1.028\n",
      "month_feb              -0.2561      0.101     -2.547      0.011      -0.453      -0.059\n",
      "month_jan              -1.3102      0.136     -9.628      0.000      -1.577      -1.043\n",
      "month_jul              -0.8949      0.085    -10.546      0.000      -1.061      -0.729\n",
      "month_jun               0.4231      0.105      4.029      0.000       0.217       0.629\n",
      "month_mar               1.5575      0.135     11.570      0.000       1.294       1.821\n",
      "month_may              -0.4610      0.081     -5.723      0.000      -0.619      -0.303\n",
      "month_nov              -0.9091      0.093     -9.755      0.000      -1.092      -0.726\n",
      "month_oct               0.8710      0.119      7.312      0.000       0.637       1.104\n",
      "month_sep               0.9047      0.132      6.828      0.000       0.645       1.164\n",
      "poutcome_other          0.3271      0.086      3.802      0.000       0.159       0.496\n",
      "poutcome_success        2.3653      0.073     32.376      0.000       2.222       2.508\n",
      "=======================================================================================\n",
      "\n",
      "Final accuracy on test set: 0.8998120092889528\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Función para entrenar un modelo de regresión logística y calcular el accuracy\n",
    "def train_and_evaluate_logit(X_train, X_test, y_train, y_test):\n",
    "    # Agregar constante al conjunto de entrenamiento\n",
    "    X_train_const = sm.add_constant(X_train)\n",
    "    \n",
    "    # Crear y ajustar el modelo de regresión logística\n",
    "    model = sm.Logit(y_train, X_train_const)\n",
    "    result = model.fit(disp=False)  # disp=False oculta los detalles de ajuste en cada iteración\n",
    "    \n",
    "    # Agregar constante al conjunto de prueba y calcular el accuracy\n",
    "    X_test_const = sm.add_constant(X_test, has_constant='add')\n",
    "    y_pred_prob = result.predict(X_test_const)\n",
    "    y_pred = [1 if p >= 0.5 else 0 for p in y_pred_prob]\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return result, accuracy\n",
    "\n",
    "# Entrenar el modelo inicial para obtener los valores p\n",
    "initial_result, initial_accuracy = train_and_evaluate_logit(X_train, X_test, y_train, y_test)\n",
    "print(\"Initial accuracy on test set:\", initial_accuracy)\n",
    "\n",
    "# Obtener las variables significativas (p < 0.05) en el modelo entrenado\n",
    "significant_vars = initial_result.pvalues[initial_result.pvalues < 0.05].index\n",
    "significant_vars = significant_vars.drop('const', errors='ignore')  # Excluir la constante si está presente\n",
    "\n",
    "# Filtrar el conjunto de datos de entrenamiento y prueba con las variables significativas\n",
    "X_train_significant = X_train[significant_vars]\n",
    "X_test_significant = X_test[significant_vars]\n",
    "\n",
    "# Reentrenar el modelo con las variables significativas\n",
    "final_result, final_accuracy = train_and_evaluate_logit(X_train_significant, X_test_significant, y_train, y_test)\n",
    "print(\"\\nFinal model summary:\")\n",
    "print(final_result.summary())\n",
    "print(\"\\nFinal accuracy on test set:\", final_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperparámetros: {'logreg__C': 0.01, 'logreg__l1_ratio': 0.5}\n",
      "Mejor AUC en validación: 0.9097856062269185\n",
      "Reporte de clasificación:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.85      0.91     11966\n",
      "           1       0.42      0.82      0.56      1598\n",
      "\n",
      "    accuracy                           0.85     13564\n",
      "   macro avg       0.70      0.84      0.73     13564\n",
      "weighted avg       0.91      0.85      0.87     13564\n",
      "\n",
      "AUC en el conjunto de prueba: 0.9097857467246057\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# Cargar el archivo\n",
    "file_path = '../Limpieza/data_dummies.csv'\n",
    "data = pd.read_csv(file_path, sep=';')\n",
    "\n",
    "# Separar variables predictoras y variable de interés\n",
    "X = data.drop(columns=['y_yes'])\n",
    "y = data['y_yes']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Crear un pipeline con estandarización y regresión logística con Elastic Net\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Estandarización de datos\n",
    "    ('logreg', LogisticRegression(penalty='elasticnet', solver='saga', max_iter=10000,class_weight='balanced'))  # Elastic Net\n",
    "])\n",
    "\n",
    "# Definir los hiperparámetros para Elastic Net\n",
    "param_grid = {\n",
    "    'logreg__l1_ratio': [0.1, 0.5, 0.7, 0.9],  # Peso entre L1 y L2 (Elastic Net)\n",
    "    'logreg__C': [0.01, 0.1, 1, 10, 100]  # Inverso de la regularización\n",
    "}\n",
    "\n",
    "# Búsqueda de hiperparámetros\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Resultados del mejor modelo\n",
    "print(\"Mejores hiperparámetros:\", grid_search.best_params_)\n",
    "print(\"Mejor AUC en validación:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Reporte de clasificación:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"AUC en el conjunto de prueba:\", roc_auc_score(y_test, y_pred_proba))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperparámetros: {'logreg__C': 0.01, 'logreg__l1_ratio': 0.9}\n",
      "Mejor AUC en validación: 0.9676247398811582\n",
      "Reporte de clasificación:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93      7952\n",
      "           1       0.52      0.47      0.49      1091\n",
      "\n",
      "    accuracy                           0.88      9043\n",
      "   macro avg       0.72      0.70      0.71      9043\n",
      "weighted avg       0.88      0.88      0.88      9043\n",
      "\n",
      "AUC en el conjunto de prueba: 0.8705527159289376\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo\n",
    "file_path = '../Limpieza/data_dummies.csv'\n",
    "data = pd.read_csv(file_path, sep=';')\n",
    "\n",
    "# Separar variables predictoras y variable de interés\n",
    "X = data.drop(columns=['y_yes'])\n",
    "y = data['y_yes']\n",
    "\n",
    "# Aplicar SMOTE después de dividir en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Aplicar SMOTE en el conjunto de entrenamiento\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Crear un pipeline con estandarización y regresión logística con Elastic Net\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Estandarización de datos\n",
    "    ('logreg', LogisticRegression(penalty='elasticnet', solver='saga', max_iter=10000))  # Elastic Net\n",
    "])\n",
    "\n",
    "# Definir los hiperparámetros para Elastic Net\n",
    "param_grid = {\n",
    "    'logreg__l1_ratio': [0.1, 0.5, 0.7, 0.9],  # Peso entre L1 y L2 (Elastic Net)\n",
    "    'logreg__C': [0.01, 0.1, 1, 10, 100]  # Inverso de la regularización\n",
    "}\n",
    "\n",
    "# Búsqueda de hiperparámetros\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "grid_search.fit(X_train_res, y_train_res)\n",
    "\n",
    "# Resultados del mejor modelo\n",
    "print(\"Mejores hiperparámetros:\", grid_search.best_params_)\n",
    "print(\"Mejor AUC en validación:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Reporte de clasificación:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"AUC en el conjunto de prueba:\", roc_auc_score(y_test, y_pred_proba))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.94     11966\n",
      "           1       0.60      0.43      0.50      1598\n",
      "\n",
      "    accuracy                           0.90     13564\n",
      "   macro avg       0.76      0.69      0.72     13564\n",
      "weighted avg       0.89      0.90      0.89     13564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar los datos\n",
    "file_path = '../Limpieza/data_dummies.csv'\n",
    "data = pd.read_csv(file_path, sep=';')\n",
    "\n",
    "# Separar variables predictoras y variable de interés\n",
    "X = data.drop(columns=['y_yes'])\n",
    "y = data['y_yes']\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Reducción de dimensionalidad con PLS (si es necesario)\n",
    "pls = PLSRegression(n_components=5)  # Puedes ajustar el número de componentes\n",
    "X_train_pls = pls.fit_transform(X_train, y_train)[0]\n",
    "X_test_pls = pls.transform(X_test)\n",
    "\n",
    "# Implementación de LDA\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train_pls, y_train)\n",
    "\n",
    "# Predicción y evaluación\n",
    "y_pred = lda.predict(X_test_pls)\n",
    "print(\"LDA Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QDA Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94     11966\n",
      "           1       0.57      0.46      0.51      1598\n",
      "\n",
      "    accuracy                           0.89     13564\n",
      "   macro avg       0.75      0.71      0.72     13564\n",
      "weighted avg       0.89      0.89      0.89     13564\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "# Implementación de QDA\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda.fit(X_train_pls, y_train)\n",
    "\n",
    "# Predicción y evaluación\n",
    "y_pred_qda = qda.predict(X_test_pls)\n",
    "print(\"QDA Classification Report:\\n\", classification_report(y_test, y_pred_qda))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de 'y_yes' en uno: 5289\n",
      "Proporción de 'y_yes' en uno: 11.70%\n"
     ]
    }
   ],
   "source": [
    "# Contar el número de valores en uno\n",
    "count_ones = y.sum()\n",
    "\n",
    "# Calcular la proporción\n",
    "proportion_ones = count_ones / len(y)\n",
    "\n",
    "print(f\"Cantidad de 'y_yes' en uno: {count_ones}\")\n",
    "print(f\"Proporción de 'y_yes' en uno: {proportion_ones:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.94     11966\n",
      "           1       0.59      0.43      0.50      1598\n",
      "\n",
      "    accuracy                           0.90     13564\n",
      "   macro avg       0.76      0.70      0.72     13564\n",
      "weighted avg       0.89      0.90      0.89     13564\n",
      "\n",
      "Coeficientes de LDA en el espacio reducido:\n",
      " [[ 1.54354385  0.84428107  0.27205597 -0.15823337 -0.12755082]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar los datos\n",
    "file_path = '../Limpieza/data_dummies.csv'\n",
    "data = pd.read_csv(file_path, sep=';')\n",
    "\n",
    "# Separar variables predictoras y variable de interés\n",
    "X = data.drop(columns=['y_yes'])\n",
    "y = data['y_yes']\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Reducción de dimensionalidad con PLS (si es necesario)\n",
    "pls = PLSRegression(n_components=5)  # Puedes ajustar el número de componentes\n",
    "X_train_pls = pls.fit_transform(X_train, y_train)[0]\n",
    "X_test_pls = pls.transform(X_test)\n",
    "\n",
    "# Ajuste de pesos para LDA\n",
    "priors = [0.88, 0.12]  # Puedes ajustar los pesos en función de la proporción de las clases\n",
    "lda = LinearDiscriminantAnalysis(priors=priors)\n",
    "lda.fit(X_train_pls, y_train)\n",
    "\n",
    "# Predicción y evaluación\n",
    "y_pred = lda.predict(X_test_pls)\n",
    "print(\"LDA Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "# Coeficientes en el espacio reducido (transformado por PLS)\n",
    "print(\"Coeficientes de LDA en el espacio reducido:\\n\", lda.coef_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes aproximados de LDA en el espacio original:\n",
      " [[ 1.07618304e-02]\n",
      " [ 1.93422510e-02]\n",
      " [ 7.70853168e-02]\n",
      " [ 1.37405646e+00]\n",
      " [-5.05569767e-02]\n",
      " [-9.03371794e-02]\n",
      " [-2.11508221e-02]\n",
      " [-3.75523268e-02]\n",
      " [-3.33979222e-02]\n",
      " [-4.23744493e-02]\n",
      " [-4.85979621e-02]\n",
      " [ 7.43989756e-02]\n",
      " [-3.61186453e-02]\n",
      " [-1.65612547e-02]\n",
      " [ 1.10770821e-01]\n",
      " [-3.63268140e-02]\n",
      " [ 3.50182135e-04]\n",
      " [-2.84252201e-02]\n",
      " [-6.95573600e-02]\n",
      " [ 6.84306031e-02]\n",
      " [ 7.19959343e-02]\n",
      " [ 5.34420746e-02]\n",
      " [ 1.27719915e-02]\n",
      " [ 5.95854364e-04]\n",
      " [-2.48237948e-01]\n",
      " [-1.01429334e-01]\n",
      " [-2.37401942e-02]\n",
      " [-3.31928598e-01]\n",
      " [-2.10058088e-01]\n",
      " [ 1.08266822e-01]\n",
      " [-1.44910539e-02]\n",
      " [-1.80651251e-01]\n",
      " [-2.17016903e-01]\n",
      " [ 8.96206712e-02]\n",
      " [ 3.25603312e-01]\n",
      " [-8.62638510e-02]\n",
      " [-1.76584242e-01]\n",
      " [ 2.59833283e-01]\n",
      " [ 2.24405932e-01]\n",
      " [ 3.08250655e-02]\n",
      " [ 8.15650967e-01]\n",
      " [-6.24098667e-02]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Obtener los coeficientes de LDA en el espacio reducido\n",
    "lda_coef_reduced = lda.coef_\n",
    "\n",
    "# Coeficientes de PLS que relacionan los componentes con las variables originales\n",
    "pls_coef = pls.x_weights_\n",
    "\n",
    "# Aproximación de los coeficientes en el espacio original\n",
    "# Multiplicamos los coeficientes de LDA por los coeficientes de PLS\n",
    "coef_original_space = np.dot(pls_coef, lda_coef_reduced.T)\n",
    "\n",
    "# Mostrar los coeficientes aproximados en el espacio original\n",
    "print(\"Coeficientes aproximados de LDA en el espacio original:\\n\", coef_original_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.94     11966\n",
      "           1       0.59      0.43      0.50      1598\n",
      "\n",
      "    accuracy                           0.90     13564\n",
      "   macro avg       0.76      0.70      0.72     13564\n",
      "weighted avg       0.89      0.90      0.89     13564\n",
      "\n",
      "Coeficientes aproximados de LDA en el espacio original:\n",
      " [[ 1.07618304e-02]\n",
      " [ 1.93422510e-02]\n",
      " [ 7.70853168e-02]\n",
      " [ 1.37405646e+00]\n",
      " [-5.05569767e-02]\n",
      " [-9.03371794e-02]\n",
      " [-2.11508221e-02]\n",
      " [-3.75523268e-02]\n",
      " [-3.33979222e-02]\n",
      " [-4.23744493e-02]\n",
      " [-4.85979621e-02]\n",
      " [ 7.43989756e-02]\n",
      " [-3.61186453e-02]\n",
      " [-1.65612547e-02]\n",
      " [ 1.10770821e-01]\n",
      " [-3.63268140e-02]\n",
      " [ 3.50182135e-04]\n",
      " [-2.84252201e-02]\n",
      " [-6.95573600e-02]\n",
      " [ 6.84306031e-02]\n",
      " [ 7.19959343e-02]\n",
      " [ 5.34420746e-02]\n",
      " [ 1.27719915e-02]\n",
      " [ 5.95854364e-04]\n",
      " [-2.48237948e-01]\n",
      " [-1.01429334e-01]\n",
      " [-2.37401942e-02]\n",
      " [-3.31928598e-01]\n",
      " [-2.10058088e-01]\n",
      " [ 1.08266822e-01]\n",
      " [-1.44910539e-02]\n",
      " [-1.80651251e-01]\n",
      " [-2.17016903e-01]\n",
      " [ 8.96206712e-02]\n",
      " [ 3.25603312e-01]\n",
      " [-8.62638510e-02]\n",
      " [-1.76584242e-01]\n",
      " [ 2.59833283e-01]\n",
      " [ 2.24405932e-01]\n",
      " [ 3.08250655e-02]\n",
      " [ 8.15650967e-01]\n",
      " [-6.24098667e-02]]\n",
      "Intervalos de confianza del 95% para los coeficientes:\n",
      "\n",
      "Variable 1: (-0.0320, 0.0400)\n",
      "Variable 2: (0.0037, 0.0655)\n",
      "Variable 3: (0.0439, 0.1138)\n",
      "Variable 4: (1.3135, 1.4351)\n",
      "Variable 5: (-0.0767, -0.0320)\n",
      "Variable 6: (-0.1499, -0.0541)\n",
      "Variable 7: (-0.0471, 0.0404)\n",
      "Variable 8: (-0.0728, -0.0184)\n",
      "Variable 9: (-0.0645, -0.0128)\n",
      "Variable 10: (-0.0823, -0.0304)\n",
      "Variable 11: (-0.0879, -0.0181)\n",
      "Variable 12: (0.0430, 0.1183)\n",
      "Variable 13: (-0.0641, -0.0093)\n",
      "Variable 14: (-0.0386, 0.0143)\n",
      "Variable 15: (0.0809, 0.1681)\n",
      "Variable 16: (-0.0537, 0.0054)\n",
      "Variable 17: (-0.0349, 0.0253)\n",
      "Variable 18: (-0.0456, 0.0107)\n",
      "Variable 19: (-0.0984, -0.0312)\n",
      "Variable 20: (0.0239, 0.0908)\n",
      "Variable 21: (0.0262, 0.0852)\n",
      "Variable 22: (0.0201, 0.0904)\n",
      "Variable 23: (-0.0097, 0.0518)\n",
      "Variable 24: (-0.0228, 0.0213)\n",
      "Variable 25: (-0.2835, -0.2106)\n",
      "Variable 26: (-0.1213, -0.0756)\n",
      "Variable 27: (-0.0650, -0.0017)\n",
      "Variable 28: (-0.3780, -0.2987)\n",
      "Variable 29: (-0.2422, -0.1646)\n",
      "Variable 30: (0.0676, 0.1669)\n",
      "Variable 31: (-0.0291, 0.0527)\n",
      "Variable 32: (-0.2127, -0.1516)\n",
      "Variable 33: (-0.2364, -0.1706)\n",
      "Variable 34: (0.0788, 0.1497)\n",
      "Variable 35: (0.2866, 0.3937)\n",
      "Variable 36: (-0.0976, -0.0375)\n",
      "Variable 37: (-0.2125, -0.1530)\n",
      "Variable 38: (0.1954, 0.3000)\n",
      "Variable 39: (0.1810, 0.2730)\n",
      "Variable 40: (-0.0187, 0.0641)\n",
      "Variable 41: (0.7531, 0.8727)\n",
      "Variable 42: (-0.0980, -0.0163)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Cargar los datos\n",
    "file_path = '../Limpieza/data_dummies.csv'\n",
    "data = pd.read_csv(file_path, sep=';')\n",
    "\n",
    "# Separar variables predictoras y variable de interés\n",
    "X = data.drop(columns=['y_yes'])\n",
    "y = data['y_yes']\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Reducción de dimensionalidad con PLS (si es necesario)\n",
    "pls = PLSRegression(n_components=5)  # Puedes ajustar el número de componentes\n",
    "X_train_pls = pls.fit_transform(X_train, y_train)[0]\n",
    "X_test_pls = pls.transform(X_test)\n",
    "\n",
    "# Ajuste de pesos para LDA\n",
    "priors = [0.88, 0.12]  # Puedes ajustar los pesos en función de la proporción de las clases\n",
    "lda = LinearDiscriminantAnalysis(priors=priors)\n",
    "lda.fit(X_train_pls, y_train)\n",
    "\n",
    "# Predicción y evaluación\n",
    "y_pred = lda.predict(X_test_pls)\n",
    "print(\"LDA Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Coeficientes en el espacio reducido (transformado por PLS)\n",
    "lda_coef_reduced = lda.coef_\n",
    "\n",
    "# Coeficientes de PLS que relacionan los componentes con las variables originales\n",
    "pls_coef = pls.x_weights_\n",
    "\n",
    "# Aproximación de los coeficientes en el espacio original\n",
    "coef_original_space = np.dot(pls_coef, lda_coef_reduced.T)\n",
    "print(\"Coeficientes aproximados de LDA en el espacio original:\\n\", coef_original_space)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Intervalos de Confianza mediante Bootstrap\n",
    "# -------------------------------------------------\n",
    "n_bootstrap = 1000\n",
    "coef_values_bootstrap = []\n",
    "\n",
    "for i in range(n_bootstrap):\n",
    "    # Remuestreo con reemplazo\n",
    "    X_resampled, y_resampled = resample(X, y, random_state=i)\n",
    "    \n",
    "    # Reducción de dimensionalidad con PLS en el conjunto resampleado\n",
    "    pls_resampled = PLSRegression(n_components=5)\n",
    "    X_resampled_pls = pls_resampled.fit_transform(X_resampled, y_resampled)[0]\n",
    "    \n",
    "    # Entrenar LDA en el espacio reducido resampleado\n",
    "    lda_resampled = LinearDiscriminantAnalysis(priors=[0.88, 0.12])\n",
    "    lda_resampled.fit(X_resampled_pls, y_resampled)\n",
    "    \n",
    "    # Obtener los coeficientes aproximados en el espacio original\n",
    "    coef_resampled_original = np.dot(pls_resampled.x_weights_, lda_resampled.coef_.T)\n",
    "    coef_values_bootstrap.append(coef_resampled_original.flatten())\n",
    "\n",
    "# Convertir a matriz para calcular intervalos de confianza\n",
    "coef_values_bootstrap = np.array(coef_values_bootstrap)\n",
    "lower_bounds = np.percentile(coef_values_bootstrap, 2.5, axis=0)\n",
    "upper_bounds = np.percentile(coef_values_bootstrap, 97.5, axis=0)\n",
    "\n",
    "print(\"Intervalos de confianza del 95% para los coeficientes:\\n\")\n",
    "for i in range(len(lower_bounds)):\n",
    "    print(f\"Variable {i+1}: ({lower_bounds[i]:.4f}, {upper_bounds[i]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Valores válidos extraídos del diccionario\n",
    "valores_validos = {\n",
    "    \"job\": [\"admin.\", \"unknown\", \"unemployed\", \"management\", \"housemaid\", \"entrepreneur\", \"student\",\n",
    "            \"blue-collar\", \"self-employed\", \"retired\", \"technician\", \"services\"],\n",
    "    \"marital\": [\"married\", \"divorced\", \"single\"],\n",
    "    \"education\": [\"unknown\", \"secondary\", \"primary\", \"tertiary\"],\n",
    "    \"default\": [\"yes\", \"no\"],\n",
    "    \"housing\": [\"yes\", \"no\"],\n",
    "    \"loan\": [\"yes\", \"no\"],\n",
    "    \"contact\": [\"unknown\", \"telephone\", \"cellular\"],\n",
    "    \"month\": [\"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\", \"nov\", \"dec\"],\n",
    "    \"poutcome\": [\"unknown\", \"other\", \"failure\", \"success\"]\n",
    "}\n",
    "\n",
    "# Rango razonable para variables numéricas\n",
    "rangos_numericos = {\n",
    "    \"age\": (18, 100),\n",
    "    \"balance\": (-10000, 100000),\n",
    "    \"day\": (1, 31),\n",
    "    \"duration\": (0, 5000),  # segundos\n",
    "    \"campaign\": (1, 50),\n",
    "    \"pdays\": (-1, 500),  # -1 significa que no fue contactado previamente\n",
    "    \"previous\": (0, 100)\n",
    "}\n",
    "\n",
    "def predecir_cliente_v2(age, balance, day, duration, campaign, pdays, previous, job=None, marital=None,\n",
    "                        education=None, default=None, housing=None, loan=None, contact=None, month=None,\n",
    "                        poutcome=None, pls_modelo=None, lda_modelo=None, columnas_X=None):\n",
    "    \"\"\"\n",
    "    Función para predecir la clase de un cliente dado sus características.\n",
    "    \"\"\"\n",
    "\n",
    "    # Crear un DataFrame vacío con las mismas columnas que el conjunto de entrenamiento X\n",
    "    cliente_df = pd.DataFrame(columns=columnas_X)\n",
    "    cliente_df.loc[0] = 0  # Inicializar todas las columnas en 0\n",
    "\n",
    "    # Validar y asignar valores numéricos\n",
    "    for var, rango in rangos_numericos.items():\n",
    "        valor = locals()[var]\n",
    "        if not (rango[0] <= valor <= rango[1]):\n",
    "            raise ValueError(f\"El valor de {var} ({valor}) está fuera del rango {rango}.\")\n",
    "        cliente_df.loc[0, var] = valor\n",
    "\n",
    "    # Validar y asignar valores categóricos\n",
    "    for var, categorias in valores_validos.items():\n",
    "        valor = locals()[var]\n",
    "        if valor is not None:\n",
    "            if valor not in categorias:\n",
    "                raise ValueError(f\"Valor '{valor}' no válido para {var}. Debe ser uno de {categorias}.\")\n",
    "            # Activar solo la columna correspondiente si está en X\n",
    "            col_name = f\"{var}_{valor}\"\n",
    "            if col_name in cliente_df.columns:\n",
    "                cliente_df.loc[0, col_name] = 1\n",
    "\n",
    "    # Convertir todos los valores a float\n",
    "    cliente_df = cliente_df.astype(float)\n",
    "\n",
    "    # Verificar que cliente_df tenga exactamente las mismas columnas y el mismo orden que X\n",
    "    if list(cliente_df.columns) != list(columnas_X):\n",
    "        columnas_extra = set(cliente_df.columns) - set(columnas_X)\n",
    "        columnas_faltantes = set(columnas_X) - set(cliente_df.columns)\n",
    "        raise ValueError(f\"Las columnas de cliente_df no coinciden con las del conjunto de entrenamiento X.\\n\"\n",
    "                         f\"Columnas adicionales en cliente_df: {columnas_extra}\\n\"\n",
    "                         f\"Columnas faltantes en cliente_df: {columnas_faltantes}\")\n",
    "\n",
    "    # Transformar las características del cliente usando el modelo PLS\n",
    "    cliente_pls = pls_modelo.transform(cliente_df.values)\n",
    "\n",
    "    # Predecir la clase con el modelo LDA\n",
    "    prediccion = lda_modelo.predict(cliente_pls)[0]\n",
    "    probabilidad = lda_modelo.predict_proba(cliente_pls)[0]\n",
    "\n",
    "    return prediccion, probabilidad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase predicha para el cliente: 0\n",
      "Probabilidad de pertenecer a cada clase: [0.97923006 0.02076994]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but PLSRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Llamada a la función con X.columns para asegurar las columnas correctas\n",
    "prediccion, probabilidad = predecir_cliente_v2(\n",
    "    age=34, balance=1500, day=15, duration=200, campaign=3, pdays=5, previous=2,\n",
    "    job=\"unemployed\", marital=\"single\", education=\"tertiary\", default=\"no\",\n",
    "    housing=\"yes\", loan=\"no\", contact=\"cellular\", month=\"may\", poutcome=\"unknown\",\n",
    "    pls_modelo=pls, lda_modelo=lda, columnas_X=X.columns\n",
    ")\n",
    "\n",
    "print(f\"Clase predicha para el cliente: {prediccion}\")\n",
    "print(f\"Probabilidad de pertenecer a cada clase: {probabilidad}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
